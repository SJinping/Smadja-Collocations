{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find specific collocations\n",
    "Find any VB-NN relation collocations\n",
    "Read file *citeseerx100000.tag.txt* in which each word is POS tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import  wordpunct_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import  wordpunct_tokenize \n",
    "\n",
    "from progressbar import AnimatedMarker, Bar, BouncingBar, ETA, \\\n",
    "    AdaptiveETA, FileTransferSpeed, FormatLabel, Percentage, \\\n",
    "    ProgressBar, ReverseBar, RotatingMarker, \\\n",
    "    SimpleProgress, Timer\n",
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "eng_symbols = '{}\"\\'()[].,:;+!?-*/&|<>=~$'\n",
    "TAGGING = {'VBZ':'VB', 'VBN':'VB', 'VBP':'VB', 'VB':'VB', 'VBG':'VB', 'NNP':'NN', 'NNS':'NN', 'NN':'NN'}\n",
    "\n",
    "def ngram_is_valid(ngram):\n",
    "    first, last = ngram[0][0], ngram[0][-1]\n",
    "    if first in eng_stopwords or last in eng_stopwords: return False\n",
    "    if any( num in first or num in last for num in '0123456789'): return False\n",
    "    if any( eng_symbol in word for word in ngram[0] for eng_symbol in eng_symbols): return False\n",
    "    return True\n",
    "\n",
    "def to_ngrams( unigrams, length):\n",
    "    tag = zip(*[zip(*unigrams)[1][i:] for i in range(length)])\n",
    "    grams = [zip(*unigrams)[0][i:] for i in range(length)]\n",
    "    grams = zip(*grams)\n",
    "    grams = zip(grams, tag)\n",
    "    return grams\n",
    "\n",
    "ngram_counts = defaultdict(Counter)\n",
    "widgets      = [FormatLabel('Processed: %(value)d records (in: %(elapsed)s)')]\n",
    "pbar         = ProgressBar(widgets = widgets)\n",
    "with open('citeseerx100000.tag.txt') as text_file:\n",
    "    for index,line in pbar(enumerate(text_file)): \n",
    "        words = line.strip().split(' ')\n",
    "        words = [(w.split('/')[0].lower(), TAGGING.get(w.split('/')[1], w.split('/')[1])) for w in words]\n",
    "        for n in range(2, max_distance + 2):\n",
    "            ngram_counts[n].update(filter(ngram_is_valid, to_ngrams(words, n)))\n",
    "    pbar.finish()\n",
    "            \n",
    "# print ngram_counts[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "skip_bigram_info = defaultdict(lambda: defaultdict(Counter))\n",
    "for dist in range(2, max_distance + 2):\n",
    "    print('Processing skip bigram for distance {}...'.format(dist))\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval = len(ngram_counts[dist].keys())).start()\n",
    "    index = 0\n",
    "    for ngram, count in ngram_counts[dist].items():\n",
    "        skip_bigram_info[ngram[0][0]][(ngram[0][-1], ngram[1][0]+'-'+ngram[1][-1])] += Counter({dist-1: count})\n",
    "        skip_bigram_info[ngram[0][-1]][(ngram[0][0], ngram[1][-1]+'-'+ngram[1][0])] += Counter({1-dist: count})\n",
    "        pbar.update(index+1)\n",
    "        index += 1\n",
    "    pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import numpy as np\n",
    "skip_bigram_abc = defaultdict(lambda: 0)\n",
    "pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval = len(skip_bigram_info.keys())).start()\n",
    "index = 0\n",
    "for word, vals in skip_bigram_info.items():\n",
    "    count = []\n",
    "    for coll, val in vals.items():\n",
    "        c = val.values()\n",
    "        c_bar = sum(c) / (2*max_distance)\n",
    "        skip_bigram_abc[(word, coll, 'freq')] = sum(c)\n",
    "        skip_bigram_abc[(word, coll, 'spread')] = (sum([x**2 for x in c]) - 2*c_bar*sum(c) + 2*max_distance*c_bar**2) / (2 * max_distance)\n",
    "        count.append(sum(c))\n",
    "    skip_bigram_abc[(word, 'avg_freq')] = np.mean(count)\n",
    "    skip_bigram_abc[(word, 'dev')] = np.std(count)\n",
    "    pbar.update(index+1)\n",
    "    index += 1\n",
    "pbar.finish()\n",
    "\n",
    "# pprint.pprint(skip_bigram_abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def skip_bigram_filter(skip_bigram_info, skip_bigram_abc):\n",
    "    cc = []\n",
    "    print('Filtering...')\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval = len(skip_bigram_info.keys())).start()\n",
    "    index = 0\n",
    "    for word, vals in skip_bigram_info.items():\n",
    "        f = skip_bigram_abc[(word, 'avg_freq')]\n",
    "        for coll, val in vals.items():\n",
    "            if skip_bigram_abc[(word, 'dev')]-0 < 1E-6:\n",
    "                strength = 0\n",
    "            else:\n",
    "                strength = (skip_bigram_abc[(word, coll, 'freq')] - f) / skip_bigram_abc[(word, 'dev')]\n",
    "            if strength < k0:\n",
    "                continue\n",
    "            spread = skip_bigram_abc[(word, coll, 'spread')]\n",
    "            if spread < U0:\n",
    "                continue\n",
    "            c_bar = sum(val.values()) / (2*max_distance)\n",
    "            peak = c_bar + k1 * math.sqrt(spread)\n",
    "            for dist, count in val.items():\n",
    "                if count >= 0:\n",
    "                    cc.append((word, coll[0], coll[1], dist, strength, spread, peak, count))\n",
    "        pbar.update(index+1)\n",
    "        index += 1\n",
    "    pbar.finish()\n",
    "    return cc\n",
    "\n",
    "cc = skip_bigram_filter(skip_bigram_info, skip_bigram_abc)\n",
    "# print cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "collocations_df = pandas.DataFrame(cc,\n",
    "                                   columns = ['base word', 'collocate', 'POS', 'distance', 'strength', 'spread', 'peak', 'p'])\n",
    "collocations_df = collocations_df.set_index(['base word', 'collocate', 'POS', 'distance']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show VB-NN relation collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collocations_df[ collocations_df.index.map(lambda x: x[2] == 'VB-NN')\n",
    "               ].sort_values(by = 'strength', ascending=False)[:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
